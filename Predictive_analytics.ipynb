{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements for part 1\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import operator\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "# Import statements for part 2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report # remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    :type y_true: numpy.ndarray\n",
    "    :type y_pred: numpy.ndarray\n",
    "    :rtype: float\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_pred[i] == y_true[i]):\n",
    "            count += 1\n",
    "    return count/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall(y_true,y_pred):\n",
    "#     :type y_true: numpy.ndarray\n",
    "#     :type y_pred: numpy.ndarray\n",
    "#     :rtype: float\n",
    "    \n",
    "    conf1 = ConfusionMatrix(y_pred, y_true)\n",
    "    conf = (np.transpose(conf1))\n",
    "    leng = len(set(y_true))\n",
    "    recall = np.zeros(leng)\n",
    "    for i in range(len(recall)):\n",
    "        val = 0\n",
    "        #print(conf[i][i])\n",
    "        #print(np.sum(conf[i]))\n",
    "        val = conf[i][i]/ np.sum(conf[i])\n",
    "        #print(val)\n",
    "        recall[i] = val\n",
    "#     print(recall)\n",
    "       \n",
    "    return (np.sum(recall)/len(recall))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    :type y_true: numpy.ndarray\n",
    "    :type y_pred: numpy.ndarray\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    conf = ConfusionMatrix(y_pred, y_true)\n",
    "\n",
    "    leng = len(set(y_true))\n",
    "    precision = np.zeros(leng)\n",
    "   \n",
    "    for i in range(len(precision)):\n",
    "        val = 0\n",
    "        #print(conf[i][i])\n",
    "        #print(np.sum(conf[i]))\n",
    "        val = conf[i][i]/ np.sum(conf[i])\n",
    "        #print(val)\n",
    "        precision[i] = val\n",
    "    #print(precision)\n",
    "       \n",
    "    return (np.sum(precision)/len(precision))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WCSS(clusters, centroids):\n",
    "    \"\"\"\n",
    "    :Clusters List[numpy.ndarray]\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    my_wcss = 0\n",
    "    no_of_clusters = len(clusters)\n",
    "    for cluster in range(no_of_clusters):\n",
    "        my_wcss += np.sum((clusters[cluster] - centroids[cluster, :]) ** 2) \n",
    "    return my_wcss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(y_pred, y_true):\n",
    "    result = []\n",
    "    leng = len(set(y_true))\n",
    "    #print(set(y_true))\n",
    "    for i in range(leng):\n",
    "        result.append(np.zeros(leng))\n",
    "       \n",
    "    for i in range(len(y_true)):\n",
    "        pred = y_pred[i]\n",
    "        actual = y_true[i]\n",
    "       \n",
    "        result[pred - 1][actual - 1] += 1\n",
    "       \n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans(x_train, no_of_clusters):\n",
    "#     References\n",
    "#     https://medium.com/machine-learning-algorithms-from-scratch/k-means-clustering-from-scratch-in-python-1675d38eee42\n",
    "#     https://mubaris.com/posts/kmeans-clustering/\n",
    "    no_of_rows = x_train.shape[0]\n",
    "    no_of_feat = x_train.shape[1]\n",
    "\n",
    "    # init centroid randomly from the dataset\n",
    "    centroid_arr = np.array([]).reshape(no_of_feat, 0)\n",
    "    for i in range(no_of_clusters):\n",
    "        centroid_arr = np.c_[centroid_arr, x_train[random.randint(0, no_of_rows - 1)]]\n",
    "    centroids_init = centroid_arr.T\n",
    "    \n",
    "    centroids_old = np.zeros((no_of_clusters, no_of_feat))\n",
    "    centroids_new = copy.deepcopy(centroids_init)\n",
    "    \n",
    "    centroid_diff = np.linalg.norm(centroids_new - centroids_old)\n",
    "    \n",
    "    distance_arr = np.zeros((no_of_rows, no_of_clusters)) # tracks distance\n",
    "    \n",
    "    while centroid_diff != 0:\n",
    "        \n",
    "        for cluster in range(no_of_clusters):\n",
    "            distance_arr[:, cluster] = np.linalg.norm(x_train - centroids_new[cluster], axis = 1)\n",
    "    \n",
    "        labels = np.argmin(distance_arr, axis = 1) # gives labels\n",
    "        \n",
    "        # updating cluster centers\n",
    "        centroids_old = copy.deepcopy(centroids_new)\n",
    "            \n",
    "        # Recalculate new centroid\n",
    "        for cluster in range(no_of_clusters):\n",
    "            centroids_new[cluster, :] = np.mean(centroids_old[cluster], axis = 0)\n",
    "            \n",
    "        centroid_diff = np.linalg.norm(centroids_new - centroids_old)\n",
    "       \n",
    "            \n",
    "    predictions = set(labels)\n",
    "    results_arr = []\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        my_index = []\n",
    "        for label in labels:\n",
    "            if label == prediction:\n",
    "                my_index = np.where(labels == label)\n",
    "        temp = x_train[my_index]\n",
    "        results_arr.append(temp)\n",
    "        \n",
    "    return results_arr, centroids_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(X_train,X_test,Y_train,n):\n",
    "#     scaler = StandardScaler()\n",
    "#     #X_norm = scaler.fit_transform(X)\n",
    "#     X_train = scaler.fit_transform(X_train)\n",
    "#     X_test = scaler.fit_transform(X_test)\n",
    "    final = []\n",
    "    #Reference: https://medium.com/@souravdey/l2-distance-matrix-vectorization-trick-26aa3247ac6c\n",
    "    dist = -2 * np.dot(X_test, X_train.T) + np.sum(X_train**2,    axis=1) + np.sum(X_test**2, axis=1)[:, np.newaxis]\n",
    "    sortDist = np.argsort(dist, axis=1)\n",
    "    filtered = sortDist[:,:n]\n",
    "    for i in filtered:\n",
    "        votes={}\n",
    "        for j in i:\n",
    "            z=Y_train.take(j)\n",
    "            if z in votes:\n",
    "                votes[z]+=1\n",
    "            else:\n",
    "                votes[z]=1\n",
    "        sortedVotes = sorted(votes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        final.append(sortedVotes[0][0])\n",
    "        #np.array(final)\n",
    "    return np.asarray(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(X_train, X):\n",
    "    #Reference: https://medium.com/@souravdey/l2-distance-matrix-vectorization-trick-26aa3247ac6c\n",
    "    dists = -2 * np.dot(X, X_train.T) + np.sum(X_train**2,    axis=1) + np.sum(X**2, axis=1)[:, np.newaxis]\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(X_train, y_train, X_test, y_test):\n",
    "#     References:\n",
    "#     https://www.youtube.com/watch?v=LDRbO9a6XPU\n",
    "#     https://www.youtube.com/watch?v=D_2LkhMJcfY&t=286s\n",
    "#     https://towardsdatascience.com/decision-tree-from-scratch-in-python-46e99dfea775\n",
    "    \n",
    "    trees = []\n",
    "    \n",
    "    X_train2 = []\n",
    "    for i in range(len(X_train)):\n",
    "        arr = np.append(X_train[i], y_train[i])\n",
    "        X_train2.append(arr)\n",
    "\n",
    "    X_test2 = []\n",
    "    for i in range(len(X_test)):\n",
    "        arr = np.append(X_test[i], y_train[i])\n",
    "        X_test2.append(arr)\n",
    "\n",
    "    X_train2 = np.asarray(X_train2)    \n",
    "    X_test2 = np.asarray(X_test2) \n",
    "        \n",
    "    parameters = math.ceil(len(X_train2[0]) ** 0.5)\n",
    "        \n",
    "    for i in range(30):\n",
    "        index = random.sample(range(0,X_train2.shape[1] - 2), parameters)\n",
    "        index.append(X_train2.shape[1] - 1)\n",
    "        data = np.asarray(random.sample(range(0,X_train2.shape[0]), 1000))\n",
    "        trees.append(build_tree(X_train2[data], index))\n",
    "        \n",
    "    pred = []\n",
    "    for row in X_test2:\n",
    "        local_pred = []\n",
    "        for tree_node in trees:\n",
    "            local_pred.append(predict(tree_node, row))\n",
    "            counts = np.bincount(local_pred)\n",
    "        pred.append(np.argmax(counts))\n",
    "    return pred       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(mytree, row):\n",
    "    if(mytree.label is not None):\n",
    "        return mytree.label\n",
    "    col = mytree.col\n",
    "    if(row[col] >= mytree.value):\n",
    "        return predict(mytree.right, row)\n",
    "    else:\n",
    "        return predict(mytree.left, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(training_data, index):\n",
    "\n",
    "    if(count(training_data)):\n",
    "        return Node(None, None, None, None, training_data[0][-1])\n",
    "            \n",
    "    true_part, false_part, info_gain, value, column = data_split(training_data, index)\n",
    "    if(info_gain == 0):\n",
    "        return Node(None, None, None, None, training_data[0][-1])\n",
    "    true_tree = build_tree(true_part, index)\n",
    "    false_tree = build_tree(false_part, index)\n",
    "    return Node(true_tree, false_tree, column, value, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(rows):\n",
    "\n",
    "    count = []\n",
    "    for row in rows:\n",
    "        if(row[-1] not in count):\n",
    "            count.append(row[-1])    \n",
    "    if (len(count) == 1):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,truebranch, falsebranch, col, value, label = None):\n",
    "        self.left = falsebranch\n",
    "        self.right = truebranch\n",
    "        self.col = col\n",
    "        self.value = value\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(rows, indices):\n",
    "    \n",
    "    current_impurity = gini_impurity(rows)\n",
    "    best_gain = -1    \n",
    "    part1 = []\n",
    "    part2 = []\n",
    "    v = 0\n",
    "    c = 0   \n",
    "    for i in indices:\n",
    "            if(indices.index(i) == len(indices) - 1):\n",
    "                pass\n",
    "            else:\n",
    "                data = []\n",
    "                for entry in rows:\n",
    "                    data.append(entry[i])            \n",
    "                for val in data:\n",
    "                    true_row, false_row = data_partition(rows, val, i)\n",
    "                    if(len(true_row) == 0 or len(false_row) == 0):\n",
    "                        continue\n",
    "                    else:\n",
    "                        gini1 = gini_impurity(true_row)\n",
    "                        gini2 = gini_impurity(false_row)\n",
    "                        total = len(true_row) + len(false_row)\n",
    "                        gini3 = ((gini1 * len(true_row))/total) + ((gini2 * len(false_row))/total)\n",
    "                        info_gain = current_impurity - gini3\n",
    "\n",
    "                        if info_gain > best_gain:\n",
    "                            best_gain = info_gain\n",
    "                            part1 = true_row\n",
    "                            part2 = false_row\n",
    "                            v = val\n",
    "                            c = i    \n",
    "    return part1, part2, best_gain, v, c   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_partition(rows, value, index):\n",
    "    true_rows = []\n",
    "    false_rows = []\n",
    "    for row in rows:\n",
    "        if row[index] >= value:\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "\n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(rows):\n",
    "    impurity = 1\n",
    "    count = {}\n",
    "    for row in rows:\n",
    "        val = row[-1]\n",
    "        if val in count.keys():\n",
    "            count[val] += 1\n",
    "        else:\n",
    "            count[val] = 1\n",
    "    \n",
    "    for key, value in count.items():\n",
    "        impure = value/len(rows)\n",
    "        impurity = impurity - impure ** 2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X_train, N):\n",
    "    # Reference\n",
    "    # https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "    c_matrix = np.cov(X_train.T)\n",
    "    solution= np.linalg.eig(c_matrix)\n",
    "    e_vals = solution[0]\n",
    "    e_vectors = solution[1]\n",
    "    index = e_vals.argsort()[::-1]\n",
    "    e_vals = e_vals[index]\n",
    "    e_vectors = e_vectors[:,index]\n",
    "    s=np.argsort(e_vals)[-N:]\n",
    "    return (np.dot(X_train,e_vectors[s].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SklearnSupervisedLearning(X_train,y_train,X_test):\n",
    "    \"\"\"\n",
    "    :type X_train: numpy.ndarray\n",
    "    :type X_test: numpy.ndarray\n",
    "    :type Y_train: numpy.ndarray\n",
    "    \n",
    "    :rtype: List[numpy.ndarray] \n",
    "    \"\"\"\n",
    "    # Logistic Regression\n",
    "    clf_lr = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "    y_pred_lr = clf_lr.predict(X_test)\n",
    "\n",
    "    \n",
    "    # SVM\n",
    "    clf_svm = svm.SVC(kernel = 'linear')\n",
    "    clf_svm.fit(x_train, y_train)\n",
    "    y_pred_svm = clf_svm.predict(x_test)\n",
    "\n",
    "    \n",
    "    # Decision Tree\n",
    "    clf_dt = tree.DecisionTreeClassifier()\n",
    "    clf_dt = clf_dt.fit(X_train, y_train)\n",
    "    y_pred_dt = clf_dt.predict(X_test)\n",
    "\n",
    "    \n",
    "    # KNN\n",
    "#     scaler = preprocessing.StandardScaler()\n",
    "#     X_train = scaler.fit_transform(X_train)\n",
    "#     X_test = scaler.fit_transform(X_test)\n",
    "    knn_model = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean', p=2)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred_knn= knn_model.predict(X_test)\n",
    "\n",
    "    return y_pred_lr, y_pred_svm, y_pred_dt, y_pred_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SklearnVotingClassifier(X_train,y_train,X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    :type X_train: numpy.ndarray\n",
    "    :type X_test: numpy.ndarray\n",
    "    :type Y_train: numpy.ndarray\n",
    "    \n",
    "    :rtype: List[numpy.ndarray] \n",
    "    \"\"\"\n",
    "    clf1 = LogisticRegression(solver = 'liblinear')\n",
    "    clf2 = tree.DecisionTreeClassifier(max_depth = 10, criterion = 'entropy')\n",
    "    clf3 = svm.SVC(kernel = 'linear')\n",
    "    clf4 = KNeighborsClassifier(n_neighbors=11, metric = 'euclidean', p = 2)\n",
    "    labels = ['Logistic Regression', 'Decision Tree', 'SVM', 'KNN']\n",
    "    hard_voting_classifier = VotingClassifier(estimators = [(labels[0], clf1),\n",
    "                                                        (labels[1], clf2),\n",
    "                                                        (labels[2], clf3),\n",
    "                                                        (labels[3], clf4)], voting = 'hard')\n",
    "    hard_voting_classifier.fit(X_train, y_train)\n",
    "    y_pred_ensemblemodel = hard_voting_classifier.predict(X_test)\n",
    "\n",
    "    return y_pred_ensemblemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data.csv')\n",
    "X = dataset.iloc[:, :-1]\n",
    "Y = dataset.iloc[:, 48]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 41)\n",
    "\n",
    "# Convert to NP array\n",
    "x_train = x_train.values\n",
    "x_test = x_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24219559.300389316\n"
     ]
    }
   ],
   "source": [
    "# K Means\n",
    "kmeans_cluster, centroids = KMeans(x_train, 11)\n",
    "my_wcss = WCSS(kmeans_cluster, centroids)\n",
    "print(my_wcss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  11.41357421875\n",
      "Recall:  11.42479901824076\n",
      "Precision:  11.487289672763193\n",
      "[[84. 76. 67. 69. 69. 82. 55. 70. 66. 68. 55.]\n",
      " [82. 78. 53. 66. 54. 63. 70. 65. 70. 59. 81.]\n",
      " [81. 65. 84. 57. 69. 60. 94. 52. 45. 67. 56.]\n",
      " [59. 59. 52. 79. 80. 65. 64. 60. 84. 47. 52.]\n",
      " [64. 88. 79. 73. 98. 64. 68. 75. 83. 61. 68.]\n",
      " [60. 84. 71. 68. 67. 82. 48. 91. 75. 68. 66.]\n",
      " [62. 56. 69. 58. 63. 72. 89. 55. 56. 58. 47.]\n",
      " [74. 59. 52. 71. 76. 69. 81. 75. 66. 74. 69.]\n",
      " [43. 72. 63. 63. 73. 72. 58. 84. 85. 65. 82.]\n",
      " [58. 87. 68. 64. 75. 76. 69. 62. 93. 87. 81.]\n",
      " [66. 54. 44. 50. 55. 61. 50. 50. 60. 43. 94.]]\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "y_pred = KNN(x_train, x_test, y_train, n=5)\n",
    "acc_knn = Accuracy(y_test, y_pred)\n",
    "print(\"Accuracy : \", acc_knn*100)\n",
    "print(\"Recall: \", Recall(y_test, y_pred) * 100)\n",
    "print(\"Precision: \", Precision(y_test, y_pred) * 100)\n",
    "\n",
    "\n",
    "print(ConfusionMatrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of PCA new shape:  (32764, 5)\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "\n",
    "pca_result = PCA(x_train, 5)\n",
    "print(\"Result of PCA new shape: \", pca_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  97.6806640625\n",
      "[0.9904502  0.97300771 0.997151   0.98189415 0.96405648 0.93342037\n",
      " 1.         0.98105548 0.96168582 0.96556671 1.        ]\n",
      "Recall:  97.71170846472046\n",
      "Precision:  97.68396627654693\n",
      "[[726.   0.   0.   0.   1.  29.   0.   0.   0.   0.   0.]\n",
      " [  0. 757.   0.   1.   0.   1.   0.   0.   4.  23.   0.]\n",
      " [  2.   0. 700.   3.   7.   0.   0.   0.   4.   0.   0.]\n",
      " [  0.   0.   0. 705.   6.   0.   0.   7.   3.   0.   0.]\n",
      " [  0.   0.   2.   8. 751.   3.   0.   4.   1.   0.   0.]\n",
      " [  2.   0.   0.   0.   2. 715.   0.   3.  15.   0.   0.]\n",
      " [  1.   0.   0.   0.   2.   0. 746.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   1.   9.   0.   0. 725.   3.   0.   0.]\n",
      " [  2.   1.   0.   0.   0.  16.   0.   0. 753.   0.   0.]\n",
      " [  0.  18.   0.   0.   0.   2.   0.   0.   0. 673.   0.]\n",
      " [  0.   2.   0.   0.   1.   0.   0.   0.   0.   1. 751.]]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf_pred = RandomForest(x_train, y_train, x_test, y_test)\n",
    "acc_rf = Accuracy(y_test, rf_pred)\n",
    "print(\"Accuracy : \", acc_rf * 100)\n",
    "print(\"Recall: \", Recall(y_test, rf_pred) * 100)\n",
    "print(\"Precision: \", Precision(y_test, rf_pred) * 100)\n",
    "\n",
    "print(ConfusionMatrix(rf_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LR:  0.55517578125\n",
      "Accuracy for SVM:  0.860595703125\n",
      "Accuracy for DT:  0.978759765625\n",
      "Accuracy for KNN:  0.1126708984375\n"
     ]
    }
   ],
   "source": [
    "# SkLearn Supervised Learning\n",
    "pred_lr, pred_svm, pred_dt, pred_knn = SklearnSupervisedLearning(x_train, y_train, x_test)\n",
    "print(\"Accuracy for LR: \", metrics.accuracy_score(y_test, pred_lr))\n",
    "print(\"Accuracy for SVM: \", metrics.accuracy_score(y_test, pred_svm))\n",
    "print(\"Accuracy for DT: \", metrics.accuracy_score(y_test, pred_dt))\n",
    "print(\"Accuracy for KNN: \", metrics.accuracy_score(y_test, pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Ensemble Model:  0.865966796875\n"
     ]
    }
   ],
   "source": [
    "# SkLearn Ensemble Model\n",
    "pred_ensemble = SklearnVotingClassifier(x_train, y_train, x_test)\n",
    "print(\"Accuracy for Ensemble Model: \", metrics.accuracy_score(y_test, pred_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
